---
layout: post
title:  On the Usage of Atomic Clocks in Spanner
date:   2022-04-1
description: Spanner use Atomic Clocks for read-only transactions. 
tags: consistency, distributed-database
categories: thoughts
---

**Draft, WIP, need proofread**

The Spanner {% cite spanner --file ref %} system is well studied since it is published 
in OSDI '12, and many geo-distributed databases (e.g., CockroachDB) following 
Spanner have been built. Everyone knows that Spanner uses
TrueTime/Atomic clocks to achieve 
strict serializability (linearizability), but a key question 
that I want to answer is *what is the role of the TrueTime API in
Spanner's protocol?* How does it contribute to Spanner's guarantee?
How should we design a system without such atomic clocks?


## Spanner in a Nutshell

Spanner is the geo-distributed database built by Google, and its technical 
paper was first published in OSDI '12. 

Spanner has a data model that is stronger than key-value stores but weaker 
than fully relational databases.  
Spanners adopts a layered design, with concurrency control (2PL) on top of 
replication (Paxos). This is a standard design easy to maintain, but it may 
cause extra RTTs. Spanner has three types of transactions, namely read-write (RW) transactions, 
consistent read-only 
transactions (RO), and stale snapshot (the namings may be 
a little bit different from its original paper). 

The key weapon of Spanner is its TrueTime API based on atomic clocks and GPS. 
The TrueTime API has a guaranteed (at least practically) offset among 
clocks. Compared to traditional non-synchronized clocks, a node 
in Spanner can predicate that a remote node's clock has passed $$T$$ 
without communication, which brings Spanner unique strength on efficiently 
ensuring consistency. 

## Spanner's Consistency/Isolation

For read-write (RW) transactions, Spanner uses (strict) 2PL for concurrency control and 
uses leader-based Paxos for replication. A RW transaction is triggered by its client: the 
client first acquire all read locks, read all values, and buffer all writes; 
then a client acquire all write locks and commit via 2PC. This is a standard 2PL and it actually 
already ensures Strict Serializability (even with replication, as all read/write requests
go through the leader) without TrueTime. A key invariant when assigning timestamp is 
that the timestamp order should be the same as the equivalent serial order of 2PL. 

For read-only (RO) transactions, Spanner use TrueTime to ensure their consistency. 
For consistency, there are two main requirements for a RO transaction $$R$$ at timestamp $$ts$$. 
First, Spanner needs to ensure that $$R$$ sees *all* transaction writes with timestamps smaller than
$$ts$$. This completeness enures *atomicity* because updates from the same transaction share the 
same timestamp, so completeness already ensures atomicity. 
The completeness also preserves *serializability*: RW transactions are serialized 
according to their timestamps, so completeness ensures there is no gap in the 
equivalent serial schedule of RW transactions. Completeness together with 
the second requirement below also ensures the strictness of serializability. 

Second, to ensure "strictness" in strict serializability, Spanner must 
ensure that a RO transaction's timestamp is larger than all completed 
RW transactions'. This is where Spanner's TrueTime comes to help. Since 
nodes' clocks have a bounded offset, consider a RO transaction $$R$$ starting
$$t$$. When $$R$$ starts, any other node's clock is at most $$t +\frac{1}{2} offset$$. 
Therefore, assigning $$R$$ with a timestamp larger than $$t +\frac{1}{2} offset$$ is 
enough for consistency. 

Without TrueTime, these two requirements become expensive to meet. For the first requirement, 
Two key aspects are (1) to ensure that nodes will not travel back in the history and assign 
transactions with past timestamps, and (2) to have a mechanism helping nodes to predicate 
that all on-the-flight transactions. Achieving these two aspects efficiently in a geo-distributed 
deployment is challenging. Ocean Vista {% cite oceanvista --file ref %} is an impressive work showing 
how these two aspects can be met with commodity hardware while still ensuring high performance. 
My previous work DAST {% cite chen2021achieving --file ref %} also addresses these two aspects, but in a 
more specific near-client computing deployment scenarios: data is assigned home region based on geo-locality, 
which brings DAST the potential for higher performance.

For the second requirement, without TrueTime, a system must keep track of the timestamp of finished 
transactions. This cannot be efficiently achieved in general cases for geo-distributed deployments, and 
normally a RO has to wait/communicate with delay in the order of global RTT time. 
This is actually in line with the SNOW {% cite snow --file ref %} impossibility result, and explains why Google invest 
to build such a strong hardware to support RO transactions. 
There are two difficulties here. First, nodes' clocks are not synchronized, so reading at a specific timestamp 
is bounded by the global straggler (who can assign relevant read-write transactions). DAST somehow 
sidestep this difficulty because in DAST, for each key $$K$$, only a restricted set of local nodes 
can assign timestamps RW transactions accessing $$K$$. Second, most systems do not explicitly record the time 
when each transaction *finishes*, so to ensure strict serializability, the system has to include a conservative 
superset of finished transactions (e.g., those who are assigned timestamps). This superset makes the RO 
must wait for un-finished transactions. 

Overall, there seems to be two chances for improving RO transactions in Strictly serializable databases:
(1) strong data ownership based on locality (e.g,. SLOG {% cite oceanvista --file ref %}), 
and (2) leveraging strongly synchronized clocks (e.g., recent works on NSDI {% cite geng2018exploiting najafi2022graham --file ref %} )

References:
----------

{% bibliography --file ref --cited --template post-ref %}
